{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Tutorial: how to use `pangeo-fish`\n",
    "\n",
    "\n",
    "**Overview.**\n",
    "\n",
    "This Jupyter notebook demonstrates how to use `pangeo-fish`.\n",
    "\n",
    "Specifically, we will fit the geolocation on the data from the study conducted by M. Gonze et al. titled \"Combining acoustic telemetry with archival tagging to investigate the spatial dynamics of the understudied pollack *Pollachius pollachius*\", accepted for publication in the Journal of Fish Biology.\n",
    "\n",
    "We will use the biologging tag \"A19124\", which was attached to pollack fish.\n",
    "\n",
    "As for the reference Earth Observation (EO) data, we consider the European Union Copernicus Marine Service Information (CMEMS) product \"NORTHWESTSHELF_ANALYSIS_FORECAST_PHY_004_013\".\n",
    "\n",
    "_NB: In addition to the Data Storage Tag (DST), the biologging data includes **teledetection by acoustic signals**, as well as the release and recapture/death information of the fish._\n",
    "\n",
    "Both the reference EO and the biologging data are publicly available, and the computations should be tractable for most standard laptops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Workflow.**\n",
    "\n",
    "Let's first summarize the key steps for running the geolocation:\n",
    "\n",
    "1. **Define the configuration:** define the required parameters for the analysis.\n",
    "2. **Compare the reference data with the DST information:** compare the data from the reference model with the biologging data. \n",
    "3. **Regrid the comparison to HEALPix:** translate the comparison into a HEALPix grid to avoid spatial distortion.\n",
    "4. **Construct the temporal emission matrix:** create a temporal emission probability distribution (_pdf_) from the transformed grid.\n",
    "5. **Construct another emission matrix with the acoustic detections:** calculate a similar model to the previous one, using this time the acoustic teledetections.\n",
    "6. **Combine and normalize the matrices:** merge and normalize the two _pdfs_.\n",
    "7. **Estimate (or _fit_) the geolocation model:** determine the parameters of the model based on the normalized emission matrix.\n",
    "8. **Compute the state probabilities and generate trajectories:** compute the fish's location probability distribution and generate subsequent trajectories.\n",
    "9. **Visualization:** visualize the evolution of the spatial probabilities over time and export the video.\n",
    "\n",
    "Throughout this tutorial, you will gain practical experience in setting up and executing a typical workflow using `pangeo-fish` such that you can then apply the tool with your use-case study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Initialization and configuration definition\n",
    "\n",
    "In this step, we prepare the execution of the analysis.\n",
    "It includes:\n",
    "- Installing the necessary packages.\n",
    "- Importing the required libraries.\n",
    "- Defining the parameters for the next stages of the workflow.\n",
    "- Configuring the cluster for distributed computing.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "tag_name = \"208992_argos\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "4",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "!pip install rich zstandard\n",
    "# !pip install \"xarray-healpy @ git+https://github.com/iaocea/xarray-healpy.git@0ffca6058f4008f4f22f076e2d60787fcf32ac82\"\n",
    "!pip install xhealpixify\n",
    "# !pip install -e ../pangeo-fish/.\n",
    "!pip install movingpandas more_itertools\n",
    "!pip install xarray #--upgrade\n",
    "!pip install xdggs\n",
    "!pip install healpix-convolution\n",
    "!pip install --upgrade \"cf-xarray>=0.10.4\"\n",
    "!pip install zarr -U\n",
    "!pip install xarray -U\n",
    "!pip install healpix_convolution -U"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5",
   "metadata": {},
   "source": [
    "!pip install -e ~/ECAP_FORK/pangeo-fish/. --no-deps\n",
    "!pip install -e ~/healpix-convolution/.\n",
    "!pip install movingpandas==0.21.3"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6",
   "metadata": {},
   "source": [
    "!pip install xdggs -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "from pint_xarray import unit_registry as ureg\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import pangeo_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# tag_name corresponds to the name of the biologging tag name (DST identification number),\n",
    "# which is also a path for storing all the information for the specific fish tagged with tag_name.\n",
    "\n",
    "# tag_save = \"228035\"\n",
    "# tag_root specifies228036 the root URL for tag data used for this computation.\n",
    "tag_root = \"s3://gfts-ifremer/fra_sodeika/tags/formatted\"\n",
    "\n",
    "# ref_url is the path to the reference model\n",
    "\n",
    "# Liste des tags de 2021\n",
    "tags_2021 = [\n",
    "    \"203226_argos\",\n",
    "    \"208991_argos\",\n",
    "    \"208992_argos\",\n",
    "    \"208992_archival\",\n",
    "    \"208993_argos\",\n",
    "]\n",
    "\n",
    "if tag_name in tags_2021:\n",
    "    ref_model_file = \"~/ECAP_FORK/light_pdf/ecap_jmarc_try/okuyama/tags/copernicus_ref_model/copernicus_jpn_daily_feb_mar_2021.zarr/\"\n",
    "else:\n",
    "    ref_model_file = \"~/ECAP_FORK/light_pdf/ecap_jmarc_try/okuyama/tags/copernicus_ref_model/copernicus_jpn_daily_20220105.zarr/\"\n",
    "# scratch_root specifies the root directory for storing output files.\n",
    "# storage_options specifies options for the filesystem storing output files.\n",
    "\n",
    "## example for remote storage\n",
    "# scratch_root = \"s3://gfts-ifremer/fra_sodeika/run/capetienne/papermill_copernicus/same_bbox\"\n",
    "scratch_root = \"s3://gfts-ifremer/fra_sodeika/run/capetienne/papermill_copernicus/same_bbox/foscat_daily/variable_std\"\n",
    "storage_options = {\n",
    "    \"anon\": False,\n",
    "    \"profile\": \"gfts\",\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "        \"region_name\": \"gra\",\n",
    "    },\n",
    "}\n",
    "## example for using your local file system instead\n",
    "# scratch_root = \".\"\n",
    "# storage_options = None\n",
    "\n",
    "# Default chunk value for time dimension.  This values depends on the configuration of your dask cluster.\n",
    "chunk_time = 1\n",
    "\n",
    "# Either to use a HEALPix grid ([\"cells\"]) or a 2D grid ([\"x\", \"y\"])\n",
    "dims = [\"cells\"]\n",
    "\n",
    "# bbox, bounding box, defines the latitude and longitude range for the analysis area.\n",
    "bbox = {\"latitude\": [16, 25], \"longitude\": [117, 130]}\n",
    "bbox = {\"latitude\": [20, 30], \"longitude\": [117, 135]}\n",
    "bbox = {\"latitude\": [16, 35], \"longitude\": [120, 145]}\n",
    "\n",
    "# bbox = {\"latitude\": [20, 35], \"longitude\": [125, 140]}\n",
    "\n",
    "# relative_depth_threshold defines the acceptable fish depth relative to the maximum tag depth.\n",
    "# It determines whether the fish can be considered to be in a certain location based on depth.\n",
    "relative_depth_threshold = 0\n",
    "\n",
    "# optional rotation for the HEALPix grid\n",
    "rot = {\"lat\": 0, \"lon\": 0}\n",
    "# nside defines the resolution of the healpix grid used for regridding.\n",
    "nside = 1024\n",
    "refinement_level = 10\n",
    "# min_vertices sets the minimum number of vertices for a valid transcription for regridding.\n",
    "min_vertices = 1\n",
    "\n",
    "# differences_std sets the standard deviation for scipy.stats.norm.pdf.\n",
    "# It expresses the estimated certainty of the field of difference.\n",
    "differences_std = 1.0\n",
    "# initial_std sets the covariance for initial event.\n",
    "# It shows the certainty of the initial area.\n",
    "initial_std = 1e-4\n",
    "# recapture_std sets the covariance for recapture event.\n",
    "# It shows the certainty of the final recapture area if it is known.\n",
    "recapture_std = 1e-4\n",
    "# earth_radius defines the radius of the Earth used for distance calculations.\n",
    "earth_radius = ureg.Quantity(6371, \"km\")\n",
    "# maximum_speed sets the maximum allowable speed for the tagged fish.\n",
    "maximum_speed = ureg.Quantity(120, \"km / day\")\n",
    "# adjustment_factor adjusts parameters for a more fuzzy search.\n",
    "# It will factor the allowed maximum displacement of the fish.\n",
    "adjustment_factor = 5\n",
    "# truncate sets the truncating factor for computed maximum allowed sigma for convolution process.\n",
    "truncate = 4\n",
    "\n",
    "# tolerance describes the tolerance level of the search during the fitting/optimization of the geolocation.\n",
    "# Smaller values will make the optimization iterate more\n",
    "tolerance = 1e-3 if dims == [\"x\", \"y\"] else 1e-4\n",
    "\n",
    "# track_modes defines the modes for generating fish's trajectories.\n",
    "track_modes = [\"mean\", \"mode\"]\n",
    "\n",
    "# additional_track_quantities sets quantities to compute for tracks using moving pandas.\n",
    "additional_track_quantities = [\"speed\", \"distance\"]\n",
    "\n",
    "\n",
    "# time_step defines the time interval between each frame of the visualization\n",
    "time_step = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define target root directories for storing analysis results.\n",
    "target_root = f\"{scratch_root}/{tag_name}\"\n",
    "\n",
    "# Defines default chunk size for optimization.\n",
    "default_chunk = {\"time\": chunk_time, \"lat\": -1, \"lon\": -1}\n",
    "default_chunk_dims = {\"time\": chunk_time}\n",
    "default_chunk_dims.update({d: -1 for d in dims})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a local cluster for distributed computing.\n",
    "from distributed import LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can start by loading the biologging data (or _tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import load_tag\n",
    "\n",
    "tag, tag_log, time_slice = load_tag(\n",
    "    tag_root=tag_root, tag_name=tag_name, storage_options=storage_options\n",
    ")\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "You can plot the time series of the DST with the function `plot_tag()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import plot_tag\n",
    "\n",
    "plot = plot_tag(\n",
    "    tag=tag,\n",
    "    tag_log=tag_log,\n",
    "    # you can directly save the plot if you want\n",
    "    save_html=True,\n",
    "    storage_options=storage_options,\n",
    "    target_root=target_root,\n",
    ")\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Compare the reference data with the DST logs\n",
    "\n",
    "In this step, we compare the reference model data with Data Storage Tag information.\n",
    "The process involves reading and cleaning the reference model, aligning time, converting depth units and subtracting the tag data from the model.\n",
    "We also illustrate how to plot and saving the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "from pangeo_fish.helpers import _open_parquet_model, load_model, prepare_dataset\n",
    "\n",
    "ref_ds = xr.open_dataset(\n",
    "    ref_model_file,\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    storage_options=None,\n",
    ")\n",
    "\n",
    "\n",
    "model = prepare_dataset(ref_ds)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.cf import bounds_to_bins\n",
    "from pangeo_fish.tags import adapt_model_time, reshape_by_bins, to_time_slice\n",
    "\n",
    "reference_model = (\n",
    "    model.sel(time=adapt_model_time(time_slice))\n",
    "    .sel(lat=slice(*bbox[\"latitude\"]), lon=slice(*bbox[\"longitude\"]))\n",
    "    .pipe(\n",
    "        lambda ds: ds.sel(\n",
    "            depth=slice(None, (tag_log[\"pressure\"].max() - ds[\"XE\"].min()).compute())\n",
    "        )\n",
    "    )\n",
    ").chunk({\"time\": chunk_time, \"lat\": -1, \"lon\": -1, \"depth\": -1})\n",
    "reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pangeo_fish.cf import bounds_to_bins\n",
    "from pangeo_fish.tags import adapt_model_time, reshape_by_bins, to_time_slice\n",
    "\n",
    "start = pd.Timestamp(tag_log[\"time\"].min().item())\n",
    "end = pd.Timestamp(tag_log[\"time\"].max().item())\n",
    "\n",
    "extended_time_slice = slice(start - pd.Timedelta(\"1D\"), end)\n",
    "\n",
    "reference_model = (\n",
    "    model.sel(time=adapt_model_time(extended_time_slice))\n",
    "    .sel(lat=slice(*bbox[\"latitude\"]), lon=slice(*bbox[\"longitude\"]))\n",
    "    .pipe(\n",
    "        lambda ds: ds.sel(\n",
    "            depth=slice(None, (tag_log[\"pressure\"].max() - ds[\"XE\"].min()).compute())\n",
    "        )\n",
    "    )\n",
    ").chunk({\"time\": chunk_time, \"lat\": -1, \"lon\": -1, \"depth\": -1})\n",
    "\n",
    "reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reshape the tag log, so that it bins to the time step of reference_model\n",
    "reshaped_tag = reshape_by_bins(\n",
    "    tag_log,\n",
    "    dim=\"time\",\n",
    "    bins=(\n",
    "        reference_model.cf.add_bounds([\"time\"], output_dim=\"bounds\")\n",
    "        .pipe(bounds_to_bins, bounds_dim=\"bounds\")\n",
    "        .get(\"time_bins\")\n",
    "    ),\n",
    "    other_dim=\"obs\",\n",
    ").chunk({\"time\": chunk_time})\n",
    "\n",
    "reshaped_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### VARIANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def compute_hist2d(results, nbins_diff=50, nbins_depth=1000, max_depth=None):\n",
    "    all_diff = np.concatenate([r[\"diff\"] for r in results]) if results else np.array([])\n",
    "    all_pres = np.concatenate([r[\"pres\"] for r in results]) if results else np.array([])\n",
    "\n",
    "    mask = np.isfinite(all_diff) & np.isfinite(all_pres)\n",
    "    all_diff, all_pres = all_diff[mask], all_pres[mask]\n",
    "\n",
    "    if all_diff.size == 0:\n",
    "        return None, None, None\n",
    "\n",
    "    # Depth limit\n",
    "    if max_depth is not None:\n",
    "        depth_mask = all_pres <= max_depth\n",
    "        all_diff, all_pres = all_diff[depth_mask], all_pres[depth_mask]\n",
    "\n",
    "    dmin, dmax = np.nanpercentile(all_diff, [1, 99])\n",
    "    dmax_abs = max(abs(dmin), abs(dmax))\n",
    "    diff_bins = np.linspace(-dmax_abs, dmax_abs, nbins_diff + 1)\n",
    "    depth_bins = np.linspace(\n",
    "        0,\n",
    "        max_depth if max_depth is not None else np.nanpercentile(all_pres, 98),\n",
    "        nbins_depth + 1,\n",
    "    )\n",
    "\n",
    "    counts, xedges, yedges = np.histogram2d(\n",
    "        all_diff, all_pres, bins=[diff_bins, depth_bins]\n",
    "    )\n",
    "    return counts, xedges, yedges\n",
    "\n",
    "\n",
    "def variance_by_depth(counts, xedges):\n",
    "    \"\"\"\n",
    "    Computes the weighted variance of ΔT for each row (depth)\n",
    "    from the 2D histogram.\n",
    "    \"\"\"\n",
    "    xcenters = 0.5 * (xedges[:-1] + xedges[1:])\n",
    "\n",
    "    weights_sum = counts.sum(axis=0)\n",
    "\n",
    "    weights_sum[weights_sum == 0] = np.nan\n",
    "\n",
    "    mean = np.nansum(counts * xcenters[:, None], axis=0) / weights_sum\n",
    "\n",
    "    var = np.nansum(counts * (xcenters[:, None] - mean) ** 2, axis=0) / weights_sum\n",
    "\n",
    "    var_norm = var / np.nanmax(var)\n",
    "\n",
    "    return var, var_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open(\"../comp_argos_model_files/results_copernicus.pkl\", \"rb\") as f:\n",
    "    results_jamstec = pickle.load(f)\n",
    "\n",
    "\n",
    "counts_j, xedges, yedges = compute_hist2d(results_copernicus, max_depth=2000)\n",
    "\n",
    "var_j, var_j_norm = variance_by_depth(counts_j, xedges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def variance_interp_function(yedges, var):\n",
    "    depth_centers = 0.5 * (yedges[:-1] + yedges[1:])\n",
    "    f_var = interp1d(\n",
    "        depth_centers, var, bounds_error=False, fill_value=(var[0], var[-1])\n",
    "    )\n",
    "    return f_var\n",
    "\n",
    "\n",
    "def std_interp_function(yedges, var):\n",
    "    depth_centers = 0.5 * (yedges[:-1] + yedges[1:])\n",
    "\n",
    "    std = np.sqrt(var)\n",
    "\n",
    "    f_std = interp1d(\n",
    "        depth_centers, std, bounds_error=False, fill_value=(std[0], std[-1])\n",
    "    )\n",
    "\n",
    "    return f_std\n",
    "\n",
    "\n",
    "def temperature_interp_function(model_depth, model_temp):\n",
    "\n",
    "    f_temp = interp1d(\n",
    "        model_depth,\n",
    "        model_temp,\n",
    "        bounds_error=False,\n",
    "        fill_value=(model_temp[0], model_temp[-1]),\n",
    "    )\n",
    "    return f_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "yedges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_var = variance_interp_function(yedges, var_j)\n",
    "f_std = std_interp_function(yedges, var_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(var_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# f_var déjà défini via variance_interp_function(yedges, var_j)\n",
    "depths = np.linspace(0, 2000, 500)\n",
    "var_profile = f_var(depths)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(var_profile, depths, color=\"tab:blue\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Variance interpolée\")\n",
    "plt.ylabel(\"Profondeur (m)\")\n",
    "plt.title(\"Profil de variance interpolée selon la profondeur\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28",
   "metadata": {},
   "source": [
    "from pangeo_fish.diff import diff_z_var\n",
    "diff = (\n",
    "    diff_z_var(\n",
    "        reference_model.isel(time=0),\n",
    "        reshaped_tag.isel(time=0),\n",
    "        f_var=f_var,\n",
    "        depth_threshold=relative_depth_threshold,\n",
    "    )\n",
    "    .assign(\n",
    "        {\n",
    "            \"XE\": reference_model[\"XE\"],\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29",
   "metadata": {},
   "source": [
    "diff.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.diff import diff_z_var\n",
    "\n",
    "diff = diff_z_var(\n",
    "    reference_model,\n",
    "    reshaped_tag,\n",
    "    var_depth=(0.5 * (yedges[:-1] + yedges[1:])),\n",
    "    var_values=var_j,\n",
    ").assign(\n",
    "    {\n",
    "        \"H0\": reference_model[\"H0\"],\n",
    "        \"XE\": reference_model[\"XE\"],\n",
    "        \"ocean_mask\": reference_model[\"H0\"].notnull(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "31",
   "metadata": {},
   "source": [
    "from pangeo_fish.diff import diff_z_var\n",
    "diff = (\n",
    "    diff_z_var(reference_model.isel(time=0), reshaped_tag.isel(time=0), f_var=f_var, depth_threshold=relative_depth_threshold)\n",
    "    .assign(\n",
    "        {\n",
    "            # \"H0\": reference_model[\"H0\"],\n",
    "            \"XE\": reference_model[\"XE\"],\n",
    "            # \"ocean_mask\": reference_model[\"H0\"].notnull(),\n",
    "        }\n",
    "    )\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diff = diff.compute()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33",
   "metadata": {},
   "source": [
    "%%time\n",
    "diff = diff.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### end test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff[\"diff\"].count([\"lat\", \"lon\"]).plot()\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff.compute().to_zarr(\n",
    "    f\"{target_root}/diff.zarr\",\n",
    "    mode=\"w\",\n",
    "    storage_options=storage_options,\n",
    "    zarr_version=2,\n",
    ")\n",
    "\n",
    "del diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. HEALPix regridding\n",
    "\n",
    "In this step, we regrid the data from above to HEALPix coordinates. \n",
    "\n",
    "This is a complex process, composed of several steps such as defining the HEALPix grid, creating the target grid and computing interpolation weights\n",
    "\n",
    "Fortunately though, `pangeo-fish` embarks high-level functions to do the work for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import open_diff_dataset, regrid_dataset\n",
    "\n",
    "# Open the previous dataset (only necessary if you resume the notebook from here)\n",
    "diff = open_diff_dataset(target_root=target_root, storage_options=storage_options)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped = regrid_dataset(\n",
    "    ds=diff, refinement_level=10, min_vertices=min_vertices, rot=rot, dims=dims\n",
    ")[0]\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Let's plot the same chart as before to check that the HEALPix regridding hasn't changed the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped[\"diff\"].count(dims).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped[\"diff\"].compute().dggs.decode(\n",
    "    {\"grid_name\": \"healpix\", \"level\": 10, \"indexing_scheme\": \"nested\"}\n",
    ").dggs.explore(alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saves the result\n",
    "reshaped.chunk(default_chunk_dims).to_zarr(\n",
    "    f\"{target_root}/diff-regridded.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    compute=True,\n",
    "    storage_options=storage_options,\n",
    "    zarr_version=2,\n",
    ")\n",
    "del reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. Compute the emission probability distribution\n",
    "\n",
    "In this step, we use the comparison result from the step above to construct the emission probability matrix.\n",
    "\n",
    "This comparison is essentially he differences between the temperature measured by the tag and the reference sea temperature. \n",
    "\n",
    "The emission probability matrix represents the likelihood of observing a specific temperature difference given the model parameters and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import compute_emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the previous dataset (only necessary if you resume the notebook from here)\n",
    "differences = xr.open_dataset(\n",
    "    f\"{target_root}/diff-regridded.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    storage_options=storage_options,\n",
    ").pipe(lambda ds: ds.merge(ds[[\"latitude\", \"longitude\"]].compute()))\n",
    "# ... and compute the emission matrices\n",
    "emission_pdf = compute_emission_pdf(\n",
    "    diff_ds=differences,\n",
    "    events_ds=tag[\"tagging_events\"].ds,\n",
    "    differences_std=differences_std,\n",
    "    initial_std=initial_std,\n",
    "    recapture_std=recapture_std,\n",
    "    dims=dims,\n",
    "    chunk_time=chunk_time,\n",
    ")[0]\n",
    "emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_pdf.pdf.compute().dggs.decode(\n",
    "    {\"grid_name\": \"healpix\", \"level\": 10, \"indexing_scheme\": \"nested\"}\n",
    ").dggs.explore(alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Whatever the temporal distribution looks like, they must **never** (i.e, at _any time step_) sum to 0.\n",
    "\n",
    "How could we check that visually? You'd have guessed it by now: similarly as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emission_pdf = emission_pdf.chunk(default_chunk_dims).persist()\n",
    "emission_pdf[\"pdf\"].count(dims).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "emission_pdf.compute().to_zarr(\n",
    "    f\"{target_root}/emission.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    "    zarr_version=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## 5. Compute and add bathy pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "from pangeo_fish.bathy import batch_compute_pdf_bathy\n",
    "from pangeo_fish.cf import bounds_to_bins\n",
    "from pangeo_fish.tags import adapt_model_time, reshape_by_bins, to_time_slice\n",
    "\n",
    "# Ton chemin cible\n",
    "zarr_path = f\"s3://gfts-ifremer/fra_sodeika/run/capetienne/papermill_copernicus/same_bbox/foscat_daily/bathy_pdf_{tag_name}.zarr\"\n",
    "\n",
    "# Ouvre un filesystem compatible avec tes storage_options (ex: S3)\n",
    "fs = fsspec.filesystem(\"s3\", **storage_options)\n",
    "bool_bathy = fs.exists(zarr_path)\n",
    "# Vérifie si le dossier/fichier Zarr existe déjà\n",
    "if bool_bathy:\n",
    "    print(f\"⚠️ Le fichier {zarr_path} existe déjà — calcul sauté.\")\n",
    "else:\n",
    "    print(f\"✅ Aucun fichier trouvé, lancement du calcul...\")\n",
    "\n",
    "\n",
    "if bool_bathy:\n",
    "    bathy_pdf = xr.open_dataset(\n",
    "        f\"s3://gfts-ifremer/fra_sodeika/run/capetienne/papermill_copernicus/same_bbox/foscat_daily/bathy_pdf_{tag_name}.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "else:\n",
    "    import healpy as hp\n",
    "    from pangeo_fish.bathy import (\n",
    "        compute_fish_histogram_bin_size,\n",
    "        compute_healpix_histogram_region_bin_size,\n",
    "    )\n",
    "\n",
    "    # Open the previous dataset (only necessary if you resume the notebook from here)\n",
    "    full_bathy = xr.open_dataset(\n",
    "        \"s3://gfts-reference-data/gebco_2024_new.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options=storage_options,\n",
    "    ).rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "    subset_bathy = full_bathy.sel(\n",
    "        {dim: slice(bounds[0], bounds[1]) for dim, bounds in bbox.items()}\n",
    "    )\n",
    "    subset_bathy\n",
    "    import numpy as np\n",
    "\n",
    "    refinement_level = refinement_level\n",
    "\n",
    "    ds_histo = compute_healpix_histogram_region_bin_size(\n",
    "        subset_bathy,\n",
    "        nside=nside,\n",
    "        max_depth_m=1800,  # <- profondeur max désirée en mètres\n",
    "        depth_bin_size=16,  # <- largeur d’un bin en mètres\n",
    "    )\n",
    "\n",
    "    hist_ids = ds_histo.cell_ids.values  # cell ids dans ton ds_histo\n",
    "    pdf_ids = emission_pdf.cell_ids.values  # cell ids dans emission_pdf\n",
    "\n",
    "    common = np.intersect1d(hist_ids, pdf_ids)\n",
    "    # isel avec masque\n",
    "    mask = np.isin(hist_ids, common)\n",
    "    ds_histo.isel(cells=np.where(mask)[0])\n",
    "\n",
    "    # Reshape the tag log, so that it bins to the time step of reference_model\n",
    "    reshaped_tag = reshape_by_bins(\n",
    "        tag_log,\n",
    "        dim=\"time\",\n",
    "        bins=(\n",
    "            reference_model.cf.add_bounds([\"time\"], output_dim=\"bounds\")\n",
    "            .pipe(bounds_to_bins, bounds_dim=\"bounds\")\n",
    "            .get(\"time_bins\")\n",
    "        ),\n",
    "        other_dim=\"obs\",\n",
    "    ).chunk({\"time\": chunk_time})\n",
    "\n",
    "    fish_hist = compute_fish_histogram_bin_size(\n",
    "        reshaped_tag, depth_max=1800, depth_bin_size=16\n",
    "    )\n",
    "\n",
    "    pdf_da_func = batch_compute_pdf_bathy(\n",
    "        ds_histo,\n",
    "        reshaped_tag,\n",
    "        target_root,\n",
    "        batch_size=5000,\n",
    "    )\n",
    "    sum_over_cells = pdf_da_func.sum(dim=\"cells\", skipna=True)  # shape (time,)\n",
    "    # Normalising\n",
    "    bathy_pdf = pdf_da_func / sum_over_cells\n",
    "\n",
    "    bathy_pdf.compute().to_zarr(\n",
    "        f\"s3://gfts-ifremer/fra_sodeika/run/capetienne/papermill_copernicus/same_bbox/foscat_daily/bathy_pdf_{tag_name}.zarr\",\n",
    "        compute=True,\n",
    "        mode=\"w\",\n",
    "        consolidated=True,\n",
    "        zarr_version=2,\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "    bathy_pdf = xr.open_dataset(\n",
    "        f\"s3://gfts-ifremer/fra_sodeika/run/capetienne/papermill_copernicus/same_bbox/foscat_daily/bathy_pdf_{tag_name}.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options=storage_options,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def normalize_pdf_by_mask(ds, mask_var=\"mask\", pdf_var=\"pdf\", tol=1e-12):\n",
    "    mask_cells = ds[mask_var].astype(bool)\n",
    "\n",
    "    mask_time = mask_cells.expand_dims(time=ds[\"time\"]).transpose(\"time\", \"cells\")\n",
    "\n",
    "    n_valid_cells = int(mask_cells.sum().compute().item())\n",
    "    if n_valid_cells == 0:\n",
    "        raise ValueError(\n",
    "            \"Le masque indique 0 cellules valides (océan). Impossible de normaliser.\"\n",
    "        )\n",
    "    sums_by_time = ds[pdf_var].where(mask_time).fillna(0).sum(dim=\"cells\")\n",
    "    sums_vals = sums_by_time.compute()\n",
    "    to_fix = (sums_vals <= tol) | np.isnan(sums_vals)\n",
    "    idxs = np.where(to_fix.values)[0]\n",
    "    times = [str(t) for t in ds[\"time\"].isel(time=idxs).values] if idxs.size else []\n",
    "    print(\n",
    "        f\"{int(to_fix.sum().item())} pas de temps non valides. Indices: {idxs.tolist()}. Times: {times}\"\n",
    "    )\n",
    "    if idxs.size == 0:\n",
    "        return ds\n",
    "    fill_per_time = xr.where(to_fix, 1.0 / float(n_valid_cells), np.nan)\n",
    "    fill_per_time = xr.DataArray(\n",
    "        fill_per_time, coords={\"time\": ds[\"time\"]}, dims=[\"time\"]\n",
    "    )\n",
    "    replacement = xr.where(mask_time, fill_per_time, np.nan)\n",
    "    ds_fixed = ds.copy()\n",
    "    ds_fixed[pdf_var] = xr.where(to_fix, replacement, ds[pdf_var])\n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_pdf[\"mask\"] = emission_pdf[\"mask\"]\n",
    "bathy_pdf_corrected = normalize_pdf_by_mask(bathy_pdf, pdf_var=\"pdf_bathy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_pdf_corrected = normalize_pdf_by_mask(emission_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_with_bathy = emission_pdf_corrected.merge(\n",
    "    bathy_pdf_corrected, compat=\"override\"\n",
    ")\n",
    "emission_with_bathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_with_bathy[\"pdf\"].count(dims).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import normalize_pdf\n",
    "\n",
    "combined_diff_bathy = normalize_pdf(\n",
    "    ds=emission_with_bathy,\n",
    "    chunks=default_chunk_dims,\n",
    "    dims=dims,\n",
    ")[0]\n",
    "combined_diff_bathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_diff_bathy[\"pdf\"].count(dims).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_diff_bathy.compute().to_zarr(\n",
    "    f\"{target_root}/emission_w_bathy_pdf_{tag_name}.zarr\",\n",
    "    compute=True,\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    zarr_version=2,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "del emission_pdf, combined_diff_bathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 7. Estimate the model's parameters\n",
    "\n",
    "It is now time to determine the parameters of the model based on the normalized emission matrix.\n",
    "\n",
    "Precisely, is consists of finding the best `sigma`, which corresponds to the standard deviation of the Brownian motion that models the fish's movement between the time steps.  \n",
    "\n",
    "To do so, in the following we:\n",
    "1. Define the lower and upper bounds for `sigma`.  \n",
    "2. Search for the best `sigma` with `optimize_pdf()`.\n",
    "3. Save the results of the search (i.e., ` sigma`), along with any additional parameters used during the optimization, a human-readable `.json` file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import optimize_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the distributions\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/emission_w_bathy_pdf_{tag_name}.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks=default_chunk_dims,\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission.pdf.compute().dggs.decode(\n",
    "    {\"grid_name\": \"healpix\", \"level\": 10, \"indexing_scheme\": \"nested\"}\n",
    ").dggs.explore(alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission = emission.dggs.decode(\n",
    "    {\"grid_name\": \"healpix\", \"level\": 10, \"indexing_scheme\": \"nested\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the parameter's bounds and search for the best value\n",
    "params = optimize_pdf(\n",
    "    ds=emission.compute(),\n",
    "    earth_radius=earth_radius,\n",
    "    adjustment_factor=adjustment_factor,\n",
    "    truncate=truncate,\n",
    "    maximum_speed=maximum_speed,\n",
    "    tolerance=tolerance,\n",
    "    dims=dims,\n",
    "    # the results can be directly saved\n",
    "    save_parameters=True,\n",
    "    storage_options=storage_options,\n",
    "    target_root=target_root,\n",
    ")\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## 8. State probabilities and Trajectories\n",
    "\n",
    "In this second to last step, we calculate the spatial probability distribution (based on the `sigma` found earlier) and further compute trajectories.\n",
    "\n",
    "_NB: the computation precisely relies on `sigma` and the combined emission pdf._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import predict_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/emission_w_bathy_pdf_{tag_name}.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks=default_chunk_dims,\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "states, trajectories = predict_positions(\n",
    "    ds=emission.dggs.decode(\n",
    "        {\"grid_name\": \"healpix\", \"level\": 10, \"indexing_scheme\": \"nested\"}\n",
    "    ),\n",
    "    target_root=target_root,\n",
    "    storage_options=storage_options,\n",
    "    chunks=default_chunk_dims,\n",
    "    track_modes=track_modes,\n",
    "    additional_track_quantities=additional_track_quantities,\n",
    "    save=True,\n",
    "    tag_name=tag_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.states.compute().dggs.decode(\n",
    "    {\"grid_name\": \"healpix\", \"level\": 10, \"indexing_scheme\": \"nested\"}\n",
    ").dggs.explore(alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "states.compute().to_zarr(\n",
    "    f\"{target_root}/states.zarr\",\n",
    "    compute=True,\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    zarr_version=2,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

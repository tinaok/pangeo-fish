{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Tutorial: how to use `pangeo-fish`\n",
    "\n",
    "\n",
    "**Overview.**\n",
    "\n",
    "This Jupyter notebook demonstrates how to use `pangeo-fish`.\n",
    "\n",
    "Specifically, we will fit the geolocation on the data from the study conducted by M. Gonze et al. titled \"Combining acoustic telemetry with archival tagging to investigate the spatial dynamics of the understudied pollack *Pollachius pollachius*\", accepted for publication in the Journal of Fish Biology.\n",
    "\n",
    "We will use the biologging tag \"A19124\", which was attached to pollack fish.\n",
    "\n",
    "As for the reference Earth Observation (EO) data, we consider the European Union Copernicus Marine Service Information (CMEMS) product \"NORTHWESTSHELF_ANALYSIS_FORECAST_PHY_004_013\".\n",
    "\n",
    "_NB: In addition to the Data Storage Tag (DST), the biologging data includes **teledetection by acoustic signals**, as well as the release and recapture/death information of the fish._\n",
    "\n",
    "Both the reference EO and the biologging data are publicly available, and the computations should be tractable for most standard laptops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Workflow.**\n",
    "\n",
    "Let's first summarize the key steps for running the geolocation:\n",
    "\n",
    "1. **Define the configuration:** define the required parameters for the analysis.\n",
    "2. **Compare the reference data with the DST information:** compare the data from the reference model with the biologging data. \n",
    "3. **Regrid the comparison to HEALPix:** translate the comparison into a HEALPix grid to avoid spatial distortion.\n",
    "4. **Construct the temporal emission matrix:** create a temporal emission probability distribution (_pdf_) from the transformed grid.\n",
    "5. **Construct another emission matrix with the acoustic detections:** calculate a similar model to the previous one, using this time the acoustic teledetections.\n",
    "6. **Combine and normalize the matrices:** merge and normalize the two _pdfs_.\n",
    "7. **Estimate (or _fit_) the geolocation model:** determine the parameters of the model based on the normalized emission matrix.\n",
    "8. **Compute the state probabilities and generate trajectories:** compute the fish's location probability distribution and generate subsequent trajectories.\n",
    "9. **Visualization:** visualize the evolution of the spatial probabilities over time and export the video.\n",
    "\n",
    "Throughout this tutorial, you will gain practical experience in setting up and executing a typical workflow using `pangeo-fish` such that you can then apply the tool with your use-case study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. Initialization and configuration definition\n",
    "\n",
    "In this step, we prepare the execution of the analysis.\n",
    "It includes:\n",
    "- Installing the necessary packages.\n",
    "- Importing the required libraries.\n",
    "- Defining the parameters for the next stages of the workflow.\n",
    "- Configuring the cluster for distributed computing.\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "!pip install rich zstandard\n",
    "# !pip install \"xarray-healpy @ git+https://github.com/iaocea/xarray-healpy.git@0ffca6058f4008f4f22f076e2d60787fcf32ac82\"\n",
    "!pip install xhealpixify\n",
    "!pip install -e ../.\n",
    "!pip install more_itertools\n",
    "!pip install movingpandas<='0.21.3'\n",
    "!pip install 'xarray<=2025.04.0'\n",
    "!pip install xdggs\n",
    "!pip install healpix-convolution\n",
    "!pip install --upgrade \"cf-xarray>=0.10.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "from pint_xarray import unit_registry as ureg\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import pangeo_fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# tag_name corresponds to the name of the biologging tag name (DST identification number),\n",
    "# which is also a path for storing all the information for the specific fish tagged with tag_name.\n",
    "tag_name = \"A19124\"\n",
    "\n",
    "# tag_root specifies the root URL for tag data used for this computation.\n",
    "tag_root = \"https://data-taos.ifremer.fr/data_tmp/cleaned/tag/\"\n",
    "\n",
    "# ref_url is the path to the reference model\n",
    "ref_url = \"https://data-taos.ifremer.fr/kerchunk/copernicus_model_NWSHELF_2022.json\"\n",
    "\n",
    "# bathy_url is the path to the bathymetry file\n",
    "bathy_url = \"https://data-taos.ifremer.fr/global_bathy/GEBCO_2024.zarr\"\n",
    "\n",
    "# scratch_root specifies the root directory for storing output files.\n",
    "# storage_options specifies options for the filesystem storing output files.\n",
    "\n",
    "## example for remote storage\n",
    "scratch_root = \"s3://destine-gfts-data-lake/demo\"\n",
    "storage_options = {\n",
    "    \"anon\": False,\n",
    "    \"profile\": \"gfts\",\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "        \"region_name\": \"gra\",\n",
    "    },\n",
    "}\n",
    "## example for using your local file system instead\n",
    "# scratch_root = \".\"\n",
    "# storage_options = None\n",
    "\n",
    "# Default chunk value for time dimension.  This values depends on the configuration of your dask cluster.\n",
    "chunk_time = 24\n",
    "\n",
    "# Either to use a HEALPix grid ([\"cells\"]) or a 2D grid ([\"x\", \"y\"])\n",
    "dims = [\"cells\"]\n",
    "\n",
    "# bbox, bounding box, defines the latitude and longitude range for the analysis area.\n",
    "bbox = {\"latitude\": [46, 51], \"longitude\": [-8, -1]}\n",
    "\n",
    "# refinement level  defines the resolution of the healpix grid used for regridding.\n",
    "refinement_level = 12  # int(log2(4096))\n",
    "nside = 4096\n",
    "# min_vertices sets the minimum number of vertices for a valid transcription for regridding.\n",
    "min_vertices = 1\n",
    "\n",
    "# differences_std sets the standard deviation for scipy.stats.norm.pdf.\n",
    "# It expresses the estimated certainty of the field of difference.\n",
    "differences_std = 0.75\n",
    "# initial_std sets the covariance for initial event.\n",
    "# It shows the certainty of the initial area.\n",
    "initial_std = 1e-6 if dims == [\"x\", \"y\"] else 1e-5\n",
    "# recapture_std sets the covariance for recapture event.\n",
    "# It shows the certainty of the final recapture area if it is known.\n",
    "recapture_std = 1e-4\n",
    "# earth_radius defines the radius of the Earth used for distance calculations.\n",
    "earth_radius = ureg.Quantity(6371, \"km\")\n",
    "# maximum_speed sets the maximum allowable speed for the tagged fish.\n",
    "maximum_speed = ureg.Quantity(60, \"km / day\")\n",
    "# adjustment_factor adjusts parameters for a more fuzzy search.\n",
    "# It will factor the allowed maximum displacement of the fish.\n",
    "adjustment_factor = 5\n",
    "# truncate sets the truncating factor for computed maximum allowed sigma for convolution process.\n",
    "truncate = 4\n",
    "\n",
    "# Maximum depth taken into account using a bathymetry file before clipping the deeper depth to this value.\n",
    "max_depth_bins = 300\n",
    "\n",
    "# receiver_buffer sets the maximum allowed detection distance for acoustic receivers.\n",
    "receiver_buffer = ureg.Quantity(1000, \"m\")\n",
    "\n",
    "\n",
    "# tolerance describes the tolerance level of the search during the fitting/optimization of the geolocation.\n",
    "# Smaller values will make the optimization iterate more\n",
    "tolerance = 1e-3 if dims == [\"x\", \"y\"] else 1e-6\n",
    "\n",
    "# track_modes defines the modes for generating fish's trajectories.\n",
    "track_modes = [\"mean\", \"mode\"]\n",
    "# additional_track_quantities sets quantities to compute for tracks using moving pandas.\n",
    "additional_track_quantities = [\"speed\", \"distance\"]\n",
    "\n",
    "\n",
    "# time_step defines the time interval between each frame of the visualization\n",
    "time_step = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define target root directories for storing analysis results.\n",
    "target_root = f\"{scratch_root}/{tag_name}\"\n",
    "\n",
    "# Defines default chunk size for optimization.\n",
    "default_chunk = {\"time\": chunk_time, \"lat\": -1, \"lon\": -1}\n",
    "default_chunk_dims = {\"time\": chunk_time}\n",
    "default_chunk_dims.update({d: -1 for d in dims})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a local cluster for distributed computing.\n",
    "from distributed import LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Now that everything is set up, we can start by loading the biologging data (or _tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import load_tag\n",
    "\n",
    "tag, tag_log, time_slice = load_tag(\n",
    "    tag_root=tag_root, tag_name=tag_name, storage_options=storage_options\n",
    ")\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "You can plot the time series of the DST with the function `plot_tag()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import plot_tag\n",
    "\n",
    "plot = plot_tag(\n",
    "    tag=tag,\n",
    "    tag_log=tag_log,\n",
    "    # you can directly save the plot if you want\n",
    "    save_html=True,\n",
    "    storage_options=storage_options,\n",
    "    target_root=target_root,\n",
    ")\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. Compare the reference data with the DST logs\n",
    "\n",
    "In this step, we compare the reference model data with Data Storage Tag information.\n",
    "The process involves reading and cleaning the reference model, aligning time, converting depth units and subtracting the tag data from the model.\n",
    "We also illustrate how to plot and saving the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import compute_diff, load_model\n",
    "\n",
    "reference_model = load_model(\n",
    "    uri=ref_url,\n",
    "    tag_log=tag_log,\n",
    "    time_slice=time_slice,\n",
    "    bbox=(bbox | {\"max_depth\": tag_log[\"pressure\"].max()}),\n",
    "    chunk_time=chunk_time,\n",
    "    remote_options={},\n",
    ")\n",
    "reference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff = compute_diff(\n",
    "    reference_model=reference_model,\n",
    "    tag_log=tag_log,\n",
    "    relative_depth_threshold=0,\n",
    "    chunk_time=chunk_time,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "_We can detect abnormal data by looking at the number of non null values for each time step._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = diff.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff[\"diff\"].count([\"lat\", \"lon\"]).plot()\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff.to_zarr(f\"{target_root}/diff.zarr\", mode=\"w\", storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. HEALPix regridding\n",
    "\n",
    "In this step, we regrid the data from above to HEALPix coordinates. \n",
    "\n",
    "This is a complex process, composed of several steps such as defining the HEALPix grid, creating the target grid and computing interpolation weights\n",
    "\n",
    "Fortunately though, `pangeo-fish` embarks high-level functions to do the work for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import open_diff_dataset, regrid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the previous dataset (only necessary if you resume the notebook from here)\n",
    "diff = open_diff_dataset(target_root=target_root, storage_options=storage_options)\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped = regrid_dataset(\n",
    "    ds=diff, refinement_level=refinement_level, min_vertices=min_vertices, dims=dims\n",
    ")[0]\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Let's plot the same chart as before to check that the HEALPix regridding hasn't changed the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reshaped[\"diff\"].count(dims).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saves the result\n",
    "reshaped.chunk(default_chunk_dims).to_zarr(\n",
    "    f\"{target_root}/diff-regridded.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    compute=True,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. Compute the emission probability distribution\n",
    "\n",
    "In this step, we use the comparison result from the step above to construct the emission probability matrix.\n",
    "\n",
    "This comparison is essentially he differences between the temperature measured by the tag and the reference sea temperature. \n",
    "\n",
    "The emission probability matrix represents the likelihood of observing a specific temperature difference given the model parameters and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import compute_emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the previous dataset (only necessary if you resume the notebook from here)\n",
    "differences = xr.open_dataset(\n",
    "    f\"{target_root}/diff-regridded.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    storage_options=storage_options,\n",
    ").pipe(lambda ds: ds.merge(ds[[\"latitude\", \"longitude\"]].compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... and compute the emission matrices\n",
    "emission_pdf = compute_emission_pdf(\n",
    "    diff_ds=reshaped,\n",
    "    events_ds=tag[\"tagging_events\"].ds,\n",
    "    differences_std=differences_std,\n",
    "    initial_std=initial_std,\n",
    "    recapture_std=recapture_std,\n",
    "    dims=dims,\n",
    "    chunk_time=chunk_time,\n",
    ")[0]\n",
    "emission_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Whatever the temporal distribution looks like, they must **never** (i.e, at _any time step_) sum to 0.\n",
    "\n",
    "How could we check that visually? You'd have guessed it by now: similarly as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "emission_pdf = emission_pdf.chunk(default_chunk_dims)\n",
    "emission_pdf[\"pdf\"].count(dims).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "emission_pdf.to_zarr(\n",
    "    f\"{target_root}/emission.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## 5. Compute a 2nd _pdf_ with the bathymetry\n",
    "\n",
    "In this step, the goal is to calculate another emission distribution, this time using a bathymetry like GEBCO, EMODNET...\n",
    "\n",
    "These additional probabilities will enhance the emission _pdf_ constructed in the previous step by incorporating information from bathymetry information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import numpy as np\n",
    "from pangeo_fish.cf import bounds_to_bins\n",
    "from pangeo_fish.tags import adapt_model_time, reshape_by_bins, to_time_slice\n",
    "from pangeo_fish.bathy import batch_compute_pdf_bathy\n",
    "import healpy as hp\n",
    "from pangeo_fish.bathy import (\n",
    "    compute_healpix_histogram_region_bin_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_path = f\"{target_root}/bathy_pdf_{tag_name}.zarr\"\n",
    "\n",
    "\n",
    "fs = fsspec.filesystem(\"s3\", **storage_options)\n",
    "bool_bathy = fs.exists(zarr_path)\n",
    "\n",
    "if bool_bathy:\n",
    "    print(f\"⚠️ Le fichier {zarr_path} existe déjà — calcul sauté.\")\n",
    "else:\n",
    "    print(f\"✅ Aucun fichier trouvé, lancement du calcul...\")\n",
    "\n",
    "\n",
    "if bool_bathy:\n",
    "    bathy_pdf = xr.open_dataset(\n",
    "        f\"{target_root}/bathy_pdf_{tag_name}.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "else:\n",
    "\n",
    "    full_bathy = xr.open_dataset(\n",
    "        \"s3://gfts-reference-data/gebco_2024_new.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options=storage_options,\n",
    "    ).rename({\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "\n",
    "    subset_bathy = full_bathy.sel(\n",
    "        {dim: slice(bounds[0], bounds[1]) for dim, bounds in bbox.items()}\n",
    "    )\n",
    "\n",
    "    ds_histo = compute_healpix_histogram_region_bin_size(\n",
    "        subset_bathy,\n",
    "        nside=nside,\n",
    "        max_depth_m=1000,  # <- profondeur max désirée en mètres\n",
    "        depth_bin_size=16,  # <- largeur d’un bin en mètres\n",
    "    )\n",
    "\n",
    "    hist_ids = ds_histo.cell_ids.values  # cell ids dans ton ds_histo\n",
    "    pdf_ids = emission_pdf.cell_ids.values  # cell ids dans emission_pdf\n",
    "\n",
    "    common = np.intersect1d(hist_ids, pdf_ids)\n",
    "    # isel avec masque\n",
    "    mask = np.isin(hist_ids, common)\n",
    "    ds_histo.isel(cells=np.where(mask)[0])\n",
    "\n",
    "    reshaped_tag = reshape_by_bins(\n",
    "        tag_log,\n",
    "        dim=\"time\",\n",
    "        bins=(\n",
    "            reference_model.cf.add_bounds([\"time\"], output_dim=\"bounds\")\n",
    "            .pipe(bounds_to_bins, bounds_dim=\"bounds\")\n",
    "            .get(\"time_bins\")\n",
    "        ),\n",
    "        other_dim=\"obs\",\n",
    "    ).chunk({\"time\": chunk_time})\n",
    "\n",
    "    pdf_da_func = batch_compute_pdf_bathy(\n",
    "        ds_histo,\n",
    "        reshaped_tag,\n",
    "        target_root,\n",
    "        batch_size=10000,\n",
    "    )\n",
    "    sum_over_cells = pdf_da_func.sum(dim=\"cells\", skipna=True)\n",
    "\n",
    "    bathy_pdf = pdf_da_func / sum_over_cells\n",
    "\n",
    "    bathy_pdf.compute().to_zarr(\n",
    "        f\"{target_root}/bathy_pdf_{tag_name}.zarr\",\n",
    "        compute=True,\n",
    "        mode=\"w\",\n",
    "        consolidated=True,\n",
    "        zarr_version=2,\n",
    "        storage_options=storage_options,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Here we regrid from latitude-longitude grid  to Healpix grid . We also genetera a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_histo = compute_healpix_histogram_region(\n",
    "    subset_bathy, nside=2**refinement_level, nb_depth_bins=max_depth_bins\n",
    ")\n",
    "\n",
    "ds_histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "bathy_pdf.pdf_bathy.compute().dggs.decode(\n",
    "    {\"grid_name\": \"healpix\", \"level\": refinement_level, \"indexing_scheme\": \"nested\"}\n",
    ").dggs.explore(alpha=0.8, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_with_bathy = emission_pdf.merge(\n",
    "    bathy_pdf.drop_indexes([\"cell_ids\"]), compat=\"override\"\n",
    ")\n",
    "emission_with_bathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import normalize_pdf\n",
    "\n",
    "combined_diff_bathy = normalize_pdf(\n",
    "    ds=emission_with_bathy,\n",
    "    chunks=default_chunk_dims,\n",
    "    dims=dims,\n",
    ")[0]\n",
    "combined_diff_bathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_diff_bathy.to_zarr(\n",
    "    f\"{target_root}/emission_w_bathy_pdf_{tag_name}.zarr\",\n",
    "    compute=True,\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 6. Compute a 3rd _pdf_ with the acoustic detections\n",
    "\n",
    "In this step, the goal is to calculate another emission distribution, this time from the acoustic detections.\n",
    "**As such, it requires the tag to include at least one detection.**\n",
    "\n",
    "These additional probabilities will enhance the emission _pdf_ constructed in the previous step by incorporating information from acoustic telemetry.\n",
    "\n",
    "_NB: we will merge and normalize the two pdfs in the next stage of the workflow._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import compute_acoustic_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the previous emission pdf and compute the emission probabilities based on acoustic detections\n",
    "emission_pdf = xr.open_dataset(\n",
    "    f\"{target_root}/emission_w_bathy_pdf_{tag_name}.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    storage_options=storage_options,\n",
    ")  # chunk?\n",
    "acoustic_pdf = compute_acoustic_pdf(\n",
    "    emission_ds=emission_pdf,\n",
    "    tag=tag,\n",
    "    receiver_buffer=receiver_buffer,\n",
    "    chunk_time=chunk_time,\n",
    "    dims=dims,\n",
    ")[0]\n",
    "acoustic_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "If you wonder how this emission matrix looks like, you can plot a combined plot of the detections and the probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tag[\"acoustic\"][\"deployment_id\"].hvplot.scatter(c=\"red\", marker=\"x\") * (\n",
    "    acoustic_pdf[\"acoustic\"] != 0\n",
    ").sum(dim=dims).hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Explanations\n",
    "On the plot above, at detection times the number of counted values drop to a few value (`5` in this example).\n",
    "\n",
    "These numbers correspond to the number of pixels that covers the detection area.\n",
    "\n",
    "Therefore, such drop is expected, since at those times we know that the fish was detected around the acoustic receivers, and so it **can't** be elsewhere.\n",
    "\n",
    "These sporadic detections will constraint a lot the geolocation model upon optimizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "**The next cell is optional. It will save the acoustic emission distribution. It is not necessary (see the next step).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "acoustic_pdf.to_zarr(\n",
    "    f\"{target_root}/acoustic.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 7. Combine and normalize the two distributions\n",
    "\n",
    "As mentioned before, before fitting the model, we need to merge the `emission` distribution with the `acoustic` one and normalize the result.\n",
    "\n",
    "The normalization ensures that the probabilities sum up to one for each time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import combine_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "combined = combine_pdfs(\n",
    "    emission_ds=emission_pdf,\n",
    "    acoustic_ds=acoustic_pdf,\n",
    "    chunks=default_chunk_dims,\n",
    "    dims=dims,\n",
    ")[0]\n",
    "combined.to_zarr(\n",
    "    f\"{target_root}/combined.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "_**Tip:** in case you don't have acoustic detection (or you don't want to use it), you can normalize the `emission_pdf` with:_\n",
    "```python\n",
    "from pangeo_fish.helpers import normalize_pdf\n",
    "combined = normalize_pdf(\n",
    "    ds=emission_pdf,\n",
    "    chunks=default_chunk_dims,\n",
    "    dims=dims,\n",
    ")[0]\n",
    "# ... and save the `combined` as shown above\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "**Let's perform a last check before fitting the model's parameters.**\n",
    "\n",
    "Indeed, remind that the _pdfs_ have been combined as their _product_.\n",
    "\n",
    "As such, for some time steps, if they don't overlap, the result will be empty (probability of 0 everywhere!).\n",
    "\n",
    "We can quickly detect if this happens by plotting the sum of the probabilities over the time dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"pdf\"].sum(dims).plot(ylim=(0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "_The sums should equal to `1`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.pdf.compute().dggs.decode(\n",
    "    {\"grid_name\": \"healpix\", \"level\": refinement_level, \"indexing_scheme\": \"nested\"}\n",
    ").dggs.explore(alpha=0.5, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 8. Estimate the model's parameters\n",
    "\n",
    "It is now time to determine the parameters of the model based on the normalized emission matrix.\n",
    "\n",
    "Precisely, is consists of finding the best `sigma`, which corresponds to the standard deviation of the Brownian motion that models the fish's movement between the time steps.  \n",
    "\n",
    "To do so, in the following we:\n",
    "1. Define the lower and upper bounds for `sigma`.  \n",
    "2. Search for the best `sigma` with `optimize_pdf()`.\n",
    "3. Save the results of the search (i.e., ` sigma`), along with any additional parameters used during the optimization, a human-readable `.json` file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import optimize_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the distributions\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/combined.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks=default_chunk_dims,\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "# Define the parameter's bounds and search for the best value\n",
    "params = optimize_pdf(\n",
    "    ds=emission,\n",
    "    earth_radius=earth_radius,\n",
    "    adjustment_factor=adjustment_factor,\n",
    "    truncate=truncate,\n",
    "    maximum_speed=maximum_speed,\n",
    "    tolerance=tolerance,\n",
    "    dims=dims,\n",
    "    # the results can be directly saved\n",
    "    save_parameters=True,\n",
    "    storage_options=storage_options,\n",
    "    target_root=target_root,\n",
    ")\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "## 9. State probabilities and Trajectories\n",
    "\n",
    "In this second to last step, we calculate the spatial probability distribution (based on the `sigma` found earlier) and further compute trajectories.\n",
    "\n",
    "_NB: the computation precisely relies on `sigma` and the combined emission pdf._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import predict_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "states, trajectories = predict_positions(\n",
    "    target_root=target_root,\n",
    "    storage_options=storage_options,\n",
    "    chunks=default_chunk_dims,\n",
    "    track_modes=track_modes,\n",
    "    additional_track_quantities=additional_track_quantities,\n",
    "    save=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Let's quickly check that the positional probability distribution `states` never sums to 0 for all timesteps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    states.sum(dims).hvplot(width=500, ylim=(0, 2), title=\"Sum of the probabilities\")\n",
    "    + states.count(dims).hvplot(width=500, title=\"Number of none-zero probabilities\")\n",
    ").opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## 10. Visualization\n",
    "\n",
    "In this final step, we visualize various aspects of the analysis results to gain insights and interpret the model outcomes. \n",
    "\n",
    "We plot the emission distribution, which represents the likelihood of observing a specific temperature difference given the model parameters and configurations. \n",
    "\n",
    "Additionally, we visualize the state probabilities, showing the likelihood of the system (i.e, the fish) being in different states (i.e, positions) at each time step. \n",
    "\n",
    "We also plot the trajectories decoded before (if you saved them).\n",
    "\n",
    "They display the possible movement patterns over time. \n",
    "\n",
    "Finally, we render the emission matrix and state probabilities in a video and store it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### 10.1 Plotting the trajectories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import plot_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_trajectories(\n",
    "    target_root=target_root,\n",
    "    track_modes=track_modes,\n",
    "    storage_options=storage_options,\n",
    "    save_html=True,\n",
    ")\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### 10.2 Plotting the `states` and `emission` distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pangeo_fish.helpers import open_distributions, render_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open_distributions(\n",
    "    target_root=target_root,\n",
    "    storage_options=storage_options,\n",
    "    chunks=default_chunk_dims,\n",
    "    chunk_time=chunk_time,\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "The interactive plot above is too large to be stored as a `HMTL` file (as done earlier with the trajectories).\n",
    "\n",
    "Fortunately, `pangeo-fish` can efficiently render images of `data` and build a video from them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_filename = render_distributions(\n",
    "    data=data,\n",
    "    output_path=f\"{target_root}/states\",\n",
    "    xlim=bbox[\"longitude\"],\n",
    "    ylim=bbox[\"latitude\"],\n",
    "    time_step=time_step,\n",
    "    extension=\"mp4\",\n",
    "    frames_dir=\"images\",\n",
    "    remove_frames=True,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
